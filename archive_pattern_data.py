#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ•°æ®å½’æ¡£ä¸å­˜å‚¨ç®¡ç†ç³»ç»Ÿ
å®Œæ•´çš„å½¢æ€æ•°æ®å½’æ¡£ã€å¤‡ä»½å’Œç®¡ç†åŠŸèƒ½
"""
import sys
import os
import json
import sqlite3
import pandas as pd
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict
import zipfile

# è®¾ç½®æ§åˆ¶å°ç¼–ç ï¼ˆWindowsï¼‰
if sys.platform.startswith('win'):
    os.system('chcp 65001 > nul')
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except AttributeError:
        pass

class PatternDataArchiver:
    """å½¢æ€æ•°æ®å½’æ¡£ç®¡ç†å™¨"""
    
    def __init__(self, base_path: str = 'output'):
        self.base_path = Path(base_path)
        self.archive_path = self.base_path / 'archives'
        self.backup_path = self.base_path / 'backups'
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.archive_path.mkdir(parents=True, exist_ok=True)
        self.backup_path.mkdir(parents=True, exist_ok=True)
        
    def create_full_archive(self) -> str:
        """åˆ›å»ºå®Œæ•´çš„æ•°æ®å½’æ¡£"""
        print("å¼€å§‹åˆ›å»ºå®Œæ•´æ•°æ®å½’æ¡£...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        archive_name = f'pattern_scout_archive_{timestamp}'
        archive_dir = self.archive_path / archive_name
        
        # åˆ›å»ºå½’æ¡£ç›®å½•ç»“æ„
        archive_dir.mkdir(exist_ok=True)
        (archive_dir / 'data').mkdir(exist_ok=True)
        (archive_dir / 'reports').mkdir(exist_ok=True)
        (archive_dir / 'charts').mkdir(exist_ok=True)
        (archive_dir / 'metadata').mkdir(exist_ok=True)
        
        # å½’æ¡£å†…å®¹ç»Ÿè®¡
        archive_stats = {
            'created_at': datetime.now().isoformat(),
            'version': '3.0',
            'description': 'PatternScout æ——å½¢å½¢æ€è¯†åˆ«å®Œæ•´å½’æ¡£',
            'contents': {}
        }
        
        # 1. å½’æ¡£æ•°æ®æ–‡ä»¶
        data_stats = self._archive_data_files(archive_dir / 'data')
        archive_stats['contents']['data'] = data_stats
        
        # 2. å½’æ¡£æŠ¥å‘Šæ–‡ä»¶
        reports_stats = self._archive_reports(archive_dir / 'reports')
        archive_stats['contents']['reports'] = reports_stats
        
        # 3. å½’æ¡£å›¾è¡¨æ–‡ä»¶
        charts_stats = self._archive_charts(archive_dir / 'charts')
        archive_stats['contents']['charts'] = charts_stats
        
        # 4. ç”Ÿæˆå…ƒæ•°æ®
        metadata_stats = self._generate_metadata(archive_dir / 'metadata')
        archive_stats['contents']['metadata'] = metadata_stats
        
        # 5. ä¿å­˜å½’æ¡£ç»Ÿè®¡ä¿¡æ¯
        with open(archive_dir / 'archive_info.json', 'w', encoding='utf-8') as f:
            json.dump(archive_stats, f, ensure_ascii=False, indent=2)
        
        # 6. åˆ›å»ºZIPå‹ç¼©åŒ…
        zip_path = self._create_zip_archive(archive_dir)
        
        print(f"âœ… å®Œæ•´å½’æ¡£åˆ›å»ºæˆåŠŸ: {zip_path}")
        return str(zip_path)
    
    def _archive_data_files(self, target_dir: Path) -> Dict:
        """å½’æ¡£æ•°æ®æ–‡ä»¶"""
        print("  å½’æ¡£æ•°æ®æ–‡ä»¶...")
        
        data_stats = {'files': [], 'total_size': 0}
        source_data_dir = self.base_path / 'data'
        
        if source_data_dir.exists():
            # å¤åˆ¶æ•°æ®åº“
            if (source_data_dir / 'patterns.db').exists():
                shutil.copy2(source_data_dir / 'patterns.db', target_dir / 'patterns.db')
                db_size = (source_data_dir / 'patterns.db').stat().st_size
                data_stats['files'].append({
                    'name': 'patterns.db',
                    'type': 'database',
                    'size': db_size,
                    'description': 'å½¢æ€è¯†åˆ«æ•°æ®åº“'
                })
                data_stats['total_size'] += db_size
            
            # å¤åˆ¶å…¶ä»–æ•°æ®æ–‡ä»¶
            for data_file in source_data_dir.rglob('*.csv'):
                if data_file.is_file():
                    rel_path = data_file.relative_to(source_data_dir)
                    target_file = target_dir / rel_path
                    target_file.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(data_file, target_file)
                    
                    file_size = data_file.stat().st_size
                    data_stats['files'].append({
                        'name': str(rel_path),
                        'type': 'csv',
                        'size': file_size,
                        'description': 'CSVæ•°æ®æ–‡ä»¶'
                    })
                    data_stats['total_size'] += file_size
        
        print(f"    å·²å½’æ¡£ {len(data_stats['files'])} ä¸ªæ•°æ®æ–‡ä»¶")
        return data_stats
    
    def _archive_reports(self, target_dir: Path) -> Dict:
        """å½’æ¡£æŠ¥å‘Šæ–‡ä»¶"""
        print("  å½’æ¡£æŠ¥å‘Šæ–‡ä»¶...")
        
        reports_stats = {'files': [], 'total_size': 0}
        source_reports_dir = self.base_path / 'reports'
        
        if source_reports_dir.exists():
            for report_file in source_reports_dir.rglob('*'):
                if report_file.is_file():
                    rel_path = report_file.relative_to(source_reports_dir)
                    target_file = target_dir / rel_path
                    target_file.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(report_file, target_file)
                    
                    file_size = report_file.stat().st_size
                    file_type = report_file.suffix.lower()
                    
                    reports_stats['files'].append({
                        'name': str(rel_path),
                        'type': file_type,
                        'size': file_size,
                        'description': self._get_file_description(report_file.name)
                    })
                    reports_stats['total_size'] += file_size
        
        print(f"    å·²å½’æ¡£ {len(reports_stats['files'])} ä¸ªæŠ¥å‘Šæ–‡ä»¶")
        return reports_stats
    
    def _archive_charts(self, target_dir: Path) -> Dict:
        """å½’æ¡£å›¾è¡¨æ–‡ä»¶"""
        print("  å½’æ¡£å›¾è¡¨æ–‡ä»¶...")
        
        charts_stats = {'files': [], 'total_size': 0}
        source_charts_dir = self.base_path / 'charts'
        
        if source_charts_dir.exists():
            for chart_file in source_charts_dir.rglob('*.png'):
                if chart_file.is_file():
                    rel_path = chart_file.relative_to(source_charts_dir)
                    target_file = target_dir / rel_path
                    target_file.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(chart_file, target_file)
                    
                    file_size = chart_file.stat().st_size
                    charts_stats['files'].append({
                        'name': str(rel_path),
                        'type': 'image/png',
                        'size': file_size,
                        'description': self._get_chart_description(chart_file.name)
                    })
                    charts_stats['total_size'] += file_size
        
        print(f"    å·²å½’æ¡£ {len(charts_stats['files'])} ä¸ªå›¾è¡¨æ–‡ä»¶")
        return charts_stats
    
    def _generate_metadata(self, target_dir: Path) -> Dict:
        """ç”Ÿæˆå…ƒæ•°æ®"""
        print("  ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶...")
        
        metadata_stats = {'files': [], 'total_size': 0}
        
        # 1. æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        db_metadata = self._generate_database_metadata()
        if db_metadata:
            db_file = target_dir / 'database_metadata.json'
            with open(db_file, 'w', encoding='utf-8') as f:
                json.dump(db_metadata, f, ensure_ascii=False, indent=2)
            
            file_size = db_file.stat().st_size
            metadata_stats['files'].append({
                'name': 'database_metadata.json',
                'type': 'metadata',
                'size': file_size,
                'description': 'æ•°æ®åº“å…ƒæ•°æ®'
            })
            metadata_stats['total_size'] += file_size
        
        # 2. åˆ†ææ‘˜è¦
        analysis_summary = self._generate_analysis_summary()
        summary_file = target_dir / 'analysis_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_summary, f, ensure_ascii=False, indent=2)
        
        file_size = summary_file.stat().st_size
        metadata_stats['files'].append({
            'name': 'analysis_summary.json',
            'type': 'summary',
            'size': file_size,
            'description': 'åˆ†æç»“æœæ‘˜è¦'
        })
        metadata_stats['total_size'] += file_size
        
        # 3. æ–‡ä»¶æ¸…å•
        inventory = self._generate_file_inventory()
        inventory_file = target_dir / 'file_inventory.json'
        with open(inventory_file, 'w', encoding='utf-8') as f:
            json.dump(inventory, f, ensure_ascii=False, indent=2)
        
        file_size = inventory_file.stat().st_size
        metadata_stats['files'].append({
            'name': 'file_inventory.json',
            'type': 'inventory',
            'size': file_size,
            'description': 'æ–‡ä»¶æ¸…å•'
        })
        metadata_stats['total_size'] += file_size
        
        print(f"    å·²ç”Ÿæˆ {len(metadata_stats['files'])} ä¸ªå…ƒæ•°æ®æ–‡ä»¶")
        return metadata_stats
    
    def _generate_database_metadata(self) -> Dict:
        """ç”Ÿæˆæ•°æ®åº“å…ƒæ•°æ®"""
        db_path = self.base_path / 'data' / 'patterns.db'
        if not db_path.exists():
            return None
            
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # è·å–è¡¨ä¿¡æ¯
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = cursor.fetchall()
            
            metadata = {
                'database_file': str(db_path),
                'created_at': datetime.now().isoformat(),
                'tables': {},
                'total_records': 0
            }
            
            for table_name, in tables:
                # è·å–è¡¨ç»“æ„
                cursor.execute(f"PRAGMA table_info({table_name})")
                columns = cursor.fetchall()
                
                # è·å–è®°å½•æ•°
                cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                record_count = cursor.fetchone()[0]
                
                metadata['tables'][table_name] = {
                    'columns': [col[1] for col in columns],
                    'record_count': record_count,
                    'structure': columns
                }
                metadata['total_records'] += record_count
            
            conn.close()
            return metadata
            
        except Exception as e:
            print(f"ç”Ÿæˆæ•°æ®åº“å…ƒæ•°æ®æ—¶å‡ºé”™: {e}")
            return None
    
    def _generate_analysis_summary(self) -> Dict:
        """ç”Ÿæˆåˆ†æç»“æœæ‘˜è¦"""
        summary = {
            'generated_at': datetime.now().isoformat(),
            'analysis_type': 'Pattern Recognition & Outcome Analysis',
            'version': 'PatternScout 3.0',
            'patterns': {},
            'outcomes': {},
            'charts': {}
        }
        
        # è¯»å–å½¢æ€åˆ†æç»“æœ
        pattern_file = self.base_path / 'reports' / 'pattern_detailed_list.csv'
        if pattern_file.exists():
            try:
                df = pd.read_csv(pattern_file)
                summary['patterns'] = {
                    'total_count': len(df),
                    'high_quality_count': len(df[df['pattern_quality'] == 'high']),
                    'average_confidence': float(df['confidence_score'].mean()),
                    'pattern_types': df['pattern_type'].value_counts().to_dict(),
                    'direction_distribution': df['flagpole_direction'].value_counts().to_dict()
                }
            except Exception as e:
                print(f"è¯»å–å½¢æ€åˆ†æç»“æœæ—¶å‡ºé”™: {e}")
        
        # è¯»å–ç»“å±€åˆ†æç»“æœ
        outcome_file = self.base_path / 'reports' / 'outcomes' / 'pattern_outcome_analysis.csv'
        if outcome_file.exists():
            try:
                df = pd.read_csv(outcome_file)
                summary['outcomes'] = {
                    'analyzed_count': len(df),
                    'breakthrough_success_rate': float(df['breakthrough_success'].mean()),
                    'average_price_move': float(df['price_move_percent'].mean()),
                    'outcome_distribution': df['outcome_type'].value_counts().to_dict()
                }
            except Exception as e:
                print(f"è¯»å–ç»“å±€åˆ†æç»“æœæ—¶å‡ºé”™: {e}")
        
        # ç»Ÿè®¡å›¾è¡¨æ–‡ä»¶
        charts_dir = self.base_path / 'charts'
        if charts_dir.exists():
            chart_files = list(charts_dir.rglob('*.png'))
            summary['charts'] = {
                'total_count': len(chart_files),
                'categories': {
                    'patterns': len(list(charts_dir.glob('patterns/*.png'))),
                    'analysis': len(list(charts_dir.glob('*.png'))),
                    'outcomes': len(list(charts_dir.glob('**/outcomes/*.png')))
                }
            }
        
        return summary
    
    def _generate_file_inventory(self) -> Dict:
        """ç”Ÿæˆå®Œæ•´æ–‡ä»¶æ¸…å•"""
        inventory = {
            'generated_at': datetime.now().isoformat(),
            'base_path': str(self.base_path),
            'directories': {},
            'total_files': 0,
            'total_size': 0
        }
        
        for root, dirs, files in os.walk(self.base_path):
            if 'archives' in root or 'backups' in root:
                continue  # è·³è¿‡å½’æ¡£å’Œå¤‡ä»½ç›®å½•
                
            root_path = Path(root)
            rel_root = root_path.relative_to(self.base_path)
            
            if str(rel_root) not in inventory['directories']:
                inventory['directories'][str(rel_root)] = {
                    'files': [],
                    'file_count': 0,
                    'total_size': 0
                }
            
            for file_name in files:
                file_path = root_path / file_name
                if file_path.is_file():
                    file_size = file_path.stat().st_size
                    
                    inventory['directories'][str(rel_root)]['files'].append({
                        'name': file_name,
                        'size': file_size,
                        'modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                        'type': file_path.suffix.lower()
                    })
                    
                    inventory['directories'][str(rel_root)]['file_count'] += 1
                    inventory['directories'][str(rel_root)]['total_size'] += file_size
                    inventory['total_files'] += 1
                    inventory['total_size'] += file_size
        
        return inventory
    
    def _create_zip_archive(self, archive_dir: Path) -> Path:
        """åˆ›å»ºZIPå‹ç¼©å½’æ¡£"""
        print("  åˆ›å»ºZIPå‹ç¼©åŒ…...")
        
        zip_path = self.archive_path / f"{archive_dir.name}.zip"
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in archive_dir.rglob('*'):
                if file_path.is_file():
                    rel_path = file_path.relative_to(archive_dir)
                    zipf.write(file_path, rel_path)
        
        # åˆ é™¤ä¸´æ—¶ç›®å½•
        shutil.rmtree(archive_dir)
        
        zip_size = zip_path.stat().st_size
        print(f"    ZIPæ–‡ä»¶å¤§å°: {zip_size / (1024*1024):.1f} MB")
        
        return zip_path
    
    def create_backup(self) -> str:
        """åˆ›å»ºæ•°æ®å¤‡ä»½"""
        print("åˆ›å»ºæ•°æ®å¤‡ä»½...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f'backup_{timestamp}'
        backup_dir = self.backup_path / backup_name
        
        # å¤‡ä»½å…³é”®æ•°æ®
        backup_dir.mkdir(exist_ok=True)
        
        backup_stats = {
            'created_at': datetime.now().isoformat(),
            'type': 'incremental_backup',
            'files_backed_up': []
        }
        
        # å¤‡ä»½æ•°æ®åº“
        db_path = self.base_path / 'data' / 'patterns.db'
        if db_path.exists():
            shutil.copy2(db_path, backup_dir / 'patterns.db')
            backup_stats['files_backed_up'].append('patterns.db')
        
        # å¤‡ä»½å…³é”®æŠ¥å‘Šæ–‡ä»¶
        reports_to_backup = [
            'reports/pattern_detailed_list.csv',
            'reports/high_quality_patterns.csv',
            'reports/outcomes/pattern_outcome_analysis.csv'
        ]
        
        for report_file in reports_to_backup:
            source_file = self.base_path / report_file
            if source_file.exists():
                target_file = backup_dir / Path(report_file).name
                shutil.copy2(source_file, target_file)
                backup_stats['files_backed_up'].append(report_file)
        
        # ä¿å­˜å¤‡ä»½ä¿¡æ¯
        with open(backup_dir / 'backup_info.json', 'w', encoding='utf-8') as f:
            json.dump(backup_stats, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_dir}")
        return str(backup_dir)
    
    def cleanup_old_archives(self, keep_days: int = 30):
        """æ¸…ç†æ—§çš„å½’æ¡£æ–‡ä»¶"""
        print(f"æ¸…ç† {keep_days} å¤©å‰çš„å½’æ¡£æ–‡ä»¶...")
        
        cutoff_date = datetime.now() - timedelta(days=keep_days)
        cleaned_count = 0
        
        for archive_file in self.archive_path.glob('*.zip'):
            file_time = datetime.fromtimestamp(archive_file.stat().st_mtime)
            if file_time < cutoff_date:
                archive_file.unlink()
                cleaned_count += 1
                print(f"  å·²åˆ é™¤: {archive_file.name}")
        
        print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ—§å½’æ¡£æ–‡ä»¶")
        return cleaned_count
    
    def _get_file_description(self, filename: str) -> str:
        """è·å–æ–‡ä»¶æè¿°"""
        descriptions = {
            'pattern_detailed_list.csv': 'è¯¦ç»†å½¢æ€è¯†åˆ«åˆ—è¡¨',
            'high_quality_patterns.csv': 'é«˜è´¨é‡å½¢æ€åˆ—è¡¨',
            'pattern_outcome_analysis.csv': 'å½¢æ€ç»“å±€åˆ†æç»“æœ',
            'pattern_statistics.png': 'å½¢æ€ç»Ÿè®¡å›¾è¡¨',
            'outcome_analysis_charts.png': 'ç»“å±€åˆ†æå›¾è¡¨'
        }
        return descriptions.get(filename, 'æ•°æ®æ–‡ä»¶')
    
    def _get_chart_description(self, filename: str) -> str:
        """è·å–å›¾è¡¨æè¿°"""
        if 'dashboard' in filename:
            return 'ç»¼åˆåˆ†æä»ªè¡¨æ¿'
        elif 'baseline' in filename:
            return 'åŠ¨æ€åŸºçº¿æ±‡æ€»å›¾è¡¨'
        elif 'pattern_' in filename:
            return 'ä¸ªåˆ«å½¢æ€åˆ†æå›¾è¡¨'
        elif 'outcome' in filename:
            return 'ç»“å±€åˆ†æå›¾è¡¨'
        else:
            return 'ç»Ÿè®¡å›¾è¡¨'

def generate_final_report():
    """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
    print("ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š...")
    
    report_content = {
        'title': 'PatternScout RBL8æœŸè´§æ——å½¢å½¢æ€è¯†åˆ«åˆ†ææŠ¥å‘Š',
        'generated_at': datetime.now().isoformat(),
        'version': '3.0',
        'summary': {},
        'detailed_results': {},
        'conclusions': [],
        'recommendations': []
    }
    
    # è¯»å–å„ä¸ªåˆ†æç»“æœ
    try:
        # å½¢æ€è¯†åˆ«ç»“æœ
        patterns_df = pd.read_csv('output/reports/pattern_detailed_list.csv')
        high_quality_df = pd.read_csv('output/reports/high_quality_patterns.csv')
        
        # ç»“å±€åˆ†æç»“æœ
        outcomes_df = pd.read_csv('output/reports/outcomes/pattern_outcome_analysis.csv')
        
        # ç”Ÿæˆæ‘˜è¦
        report_content['summary'] = {
            'total_patterns_identified': len(patterns_df),
            'high_quality_patterns': len(high_quality_df),
            'average_confidence_score': float(patterns_df['confidence_score'].mean()),
            'patterns_analyzed_for_outcome': len(outcomes_df),
            'breakthrough_success_rate': float(outcomes_df['breakthrough_success'].mean() * 100),
            'average_price_movement': float(outcomes_df['price_move_percent'].mean())
        }
        
        # è¯¦ç»†ç»“æœ
        report_content['detailed_results'] = {
            'pattern_distribution': {
                'by_type': patterns_df['pattern_type'].value_counts().to_dict(),
                'by_quality': patterns_df['pattern_quality'].value_counts().to_dict(),
                'by_direction': patterns_df['flagpole_direction'].value_counts().to_dict()
            },
            'outcome_analysis': {
                'outcome_types': outcomes_df['outcome_type'].value_counts().to_dict(),
                'success_metrics': {
                    'breakthrough_success_rate': float(outcomes_df['breakthrough_success'].mean() * 100),
                    'volume_confirmation_rate': float(outcomes_df['volume_confirm'].mean() * 100),
                    'average_time_to_outcome': float(outcomes_df['time_to_outcome'].mean())
                }
            }
        }
        
        # ç”Ÿæˆç»“è®ºå’Œå»ºè®®
        report_content['conclusions'] = [
            f"åœ¨RBL8æœŸè´§15åˆ†é’Ÿæ•°æ®ä¸­è¯†åˆ«å‡º{len(patterns_df)}ä¸ªæ——å½¢æ¨¡å¼",
            f"å…¶ä¸­{len(high_quality_df)}ä¸ªä¸ºé«˜è´¨é‡æ¨¡å¼ï¼Œå æ¯”{len(high_quality_df)/len(patterns_df)*100:.1f}%",
            f"å¹³å‡ç½®ä¿¡åº¦ä¸º{patterns_df['confidence_score'].mean():.3f}ï¼Œæ˜¾ç¤ºè¯†åˆ«è´¨é‡è¾ƒå¥½",
            f"çªç ´æˆåŠŸç‡è¾¾åˆ°{outcomes_df['breakthrough_success'].mean()*100:.1f}%ï¼Œè¡¨æ˜å½¢æ€æœ‰æ•ˆæ€§è‰¯å¥½",
            f"ä¸‰è§’æ——å½¢å ä¸»å¯¼åœ°ä½ï¼Œå æ¯”{patterns_df[patterns_df['pattern_type']=='pennant'].shape[0]/len(patterns_df)*100:.1f}%"
        ]
        
        report_content['recommendations'] = [
            "é‡ç‚¹å…³æ³¨ç½®ä¿¡åº¦0.8ä»¥ä¸Šçš„é«˜è´¨é‡å½¢æ€",
            "ç»“åˆæˆäº¤é‡ç¡®è®¤ä¿¡å·æé«˜äº¤æ˜“æˆåŠŸç‡",
            "åœ¨å½¢æ€çªç ´ååŠæ—¶è·Ÿè¿›ï¼Œé¿å…é”™è¿‡æœ€ä½³å…¥åœºæ—¶æœº",
            "å»ºè®®å°†è¯†åˆ«ç³»ç»Ÿé›†æˆåˆ°å®æ—¶äº¤æ˜“ç­–ç•¥ä¸­",
            "å®šæœŸæ›´æ–°å’Œä¼˜åŒ–è¯†åˆ«å‚æ•°ä»¥é€‚åº”å¸‚åœºå˜åŒ–"
        ]
        
    except Exception as e:
        print(f"è¯»å–åˆ†æç»“æœæ—¶å‡ºé”™: {e}")
        return None
    
    # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
    report_path = 'output/reports/final_analysis_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report_content, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æœ€ç»ˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    return report_path

def main():
    """ä¸»å‡½æ•°"""
    print("=== æ•°æ®å½’æ¡£ä¸å­˜å‚¨ç®¡ç†ç³»ç»Ÿ ===")
    
    # åˆ›å»ºå½’æ¡£ç®¡ç†å™¨
    archiver = PatternDataArchiver()
    
    # 1. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    final_report = generate_final_report()
    
    # 2. åˆ›å»ºå®Œæ•´å½’æ¡£
    archive_path = archiver.create_full_archive()
    
    # 3. åˆ›å»ºå¤‡ä»½
    backup_path = archiver.create_backup()
    
    # 4. æ¸…ç†æ—§å½’æ¡£ï¼ˆå¯é€‰ï¼‰
    # archiver.cleanup_old_archives(keep_days=30)
    
    print("\n=== å½’æ¡£å®Œæˆç»Ÿè®¡ ===")
    print(f"âœ… æœ€ç»ˆåˆ†ææŠ¥å‘Š: {final_report}")
    print(f"âœ… å®Œæ•´å½’æ¡£æ–‡ä»¶: {archive_path}")
    print(f"âœ… æ•°æ®å¤‡ä»½: {backup_path}")
    print("\nğŸ‰ æ‰€æœ‰æ•°æ®å½’æ¡£å·¥ä½œå®Œæˆï¼")

if __name__ == "__main__":
    main()